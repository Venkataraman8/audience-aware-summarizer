{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install evaluate"
      ],
      "metadata": {
        "id": "ellVwRpXplfA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b0c99bc-ede4-40dd-8436-dd0c3809970b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.12/dist-packages (0.4.6)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2025.11.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install rouge_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VqSXXupqr7k",
        "outputId": "c749043d-f5e1-46c7-c34b-ad7767635097"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.12/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge_score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_n70_LoOEgTy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25c9ec5e-2f03-4aeb-b082-ef91fc9dc09b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import html\n",
        "from typing import Dict, List\n",
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "import evaluate\n",
        "from datasets import load_dataset, DatasetDict\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPsw4-UIIowa"
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yR5SWIr4IsEd"
      },
      "outputs": [],
      "source": [
        "model_name = os.environ.get(\"T5_MODEL_NAME\", \"t5-base\")\n",
        "dataset_config = \"3.0.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Juqfa_8hIv38"
      },
      "outputs": [],
      "source": [
        "# Sequence lengths\n",
        "max_source_length = 512\n",
        "max_target_length = 128\n",
        "val_max_target_length = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XsGykkGIzgK"
      },
      "outputs": [],
      "source": [
        "# Batch / optimization\n",
        "per_device_train_batch_size = 4\n",
        "per_device_eval_batch_size = 2\n",
        "grad_accum_steps = 4\n",
        "num_train_epochs = 3\n",
        "learning_rate = 1e-5\n",
        "weight_decay = 0.01\n",
        "warmup_ratio = 0.03\n",
        "lr_scheduler = \"cosine\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWA2a-GuI3NL"
      },
      "outputs": [],
      "source": [
        "# Mixed precision\n",
        "mixed_precision = \"fp16\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7NMEMyTI6Eg"
      },
      "outputs": [],
      "source": [
        "# Preprocessing knobs\n",
        "MIN_SRC_CHARS, MIN_TGT_CHARS = 120, 10\n",
        "MAX_SRC_CHARS = 5000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ed6CFXLoI8Tn"
      },
      "outputs": [],
      "source": [
        "NUM_PROC = None\n",
        "gen_num_beams = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTqLxfb0pUAs"
      },
      "outputs": [],
      "source": [
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "345W4rxNI_k4"
      },
      "outputs": [],
      "source": [
        "BOILERPLATE_PATTERNS = [\n",
        "    r\"^Editor.?s Note:.*$\",\n",
        "    r\"^READ:\\s.*$\",\n",
        "    r\"^WATCH:\\s.*$\",\n",
        "    r\"^\\(CNN\\)\\s*[-–—]?\\s*\"\n",
        "]\n",
        "BP_RE = [re.compile(p, re.IGNORECASE | re.MULTILINE) for p in BOILERPLATE_PATTERNS]\n",
        "URL_RE = re.compile(r\"https?://\\S+\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_REUvijJQor"
      },
      "outputs": [],
      "source": [
        "def clean_text(t: str) -> str:\n",
        "    t = html.unescape(t)\n",
        "    t = URL_RE.sub(\"\", t)\n",
        "    for pat in BP_RE:\n",
        "        t = pat.sub(\"\", t)\n",
        "    t = t.replace(\"\\u00A0\", \" \")\n",
        "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
        "    return t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fk6LjVnXJUD8"
      },
      "outputs": [],
      "source": [
        "def cleaner_batch(batch):\n",
        "    articles = [clean_text(a) for a in batch[\"article\"]]\n",
        "    highlights = [clean_text(h) for h in batch[\"highlights\"]]\n",
        "    return {\"article\": articles, \"highlights\": highlights}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWgPBb6sJWIS",
        "outputId": "75068009-6abd-4963-f022-bbc7bd23d6af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading CNN/DailyMail...\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading CNN/DailyMail...\")\n",
        "raw = load_dataset(\"cnn_dailymail\", dataset_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGNfCVaaoH-5",
        "outputId": "017dbef1-f6eb-48f7-fee3-70e0a6820369"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selecting 2500 training examples...\n",
            "Selecting 500 validation examples...\n",
            "Selecting 500 testing examples...\n"
          ]
        }
      ],
      "source": [
        "# --- Define desired sizes ---\n",
        "train_size = 15000\n",
        "validation_size = 3000\n",
        "test_size = 2000\n",
        "\n",
        "# --- Select subsets ---\n",
        "print(f\"Selecting {train_size} training examples...\")\n",
        "train_subset = raw[\"train\"].select(range(train_size))\n",
        "\n",
        "print(f\"Selecting {validation_size} validation examples...\")\n",
        "validation_subset = raw[\"validation\"].select(range(validation_size))\n",
        "\n",
        "print(f\"Selecting {test_size} testing examples...\")\n",
        "test_subset = raw[\"test\"].select(range(test_size))\n",
        "\n",
        "# --- Create a new DatasetDict with the subsets ---\n",
        "raw_subset = DatasetDict({\n",
        "    \"train\": train_subset,\n",
        "    \"validation\": validation_subset,\n",
        "    \"test\": test_subset\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AT4i95dpNP1",
        "outputId": "ddc0b82f-5bb7-4382-d10d-06e966602e52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "New subset sizes:\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['article', 'highlights', 'id'],\n",
            "        num_rows: 2500\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['article', 'highlights', 'id'],\n",
            "        num_rows: 500\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['article', 'highlights', 'id'],\n",
            "        num_rows: 500\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "# --- Verify the sizes ---\n",
        "print(\"\\nNew subset sizes:\")\n",
        "print(raw_subset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUVNpJ4oJaR8"
      },
      "outputs": [],
      "source": [
        "def len_filter(ex):\n",
        "    return (len(ex[\"article\"]) >= MIN_SRC_CHARS) and (len(ex[\"highlights\"]) >= MIN_TGT_CHARS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNl-x16pJckk"
      },
      "outputs": [],
      "source": [
        "raw_subset = raw_subset.filter(len_filter, num_proc=NUM_PROC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kzv7F0ncJfj-"
      },
      "outputs": [],
      "source": [
        "def preclip_batch(batch):\n",
        "    arts = [a[:MAX_SRC_CHARS] for a in batch[\"article\"]]\n",
        "    return {\"article\": arts, \"highlights\": batch[\"highlights\"]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciTRgthbJiG7"
      },
      "outputs": [],
      "source": [
        "raw_subset = raw_subset.map(preclip_batch, batched=True, num_proc=NUM_PROC, desc=\"Pre-clipping articles\")\n",
        "raw_subset = raw_subset.map(cleaner_batch, batched=True, num_proc=NUM_PROC, desc=\"Cleaning text\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7lx90YlJkNS"
      },
      "outputs": [],
      "source": [
        "def non_empty(ex):\n",
        "    return (len(ex[\"article\"]) > 0) and (len(ex[\"highlights\"]) > 0)\n",
        "\n",
        "raw_subset = raw_subset.filter(non_empty, num_proc=NUM_PROC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzNABAnpJm_J",
        "outputId": "20183eaa-ddd3-4839-fce6-ab8f3d1c5e1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model/tokenizer: t5-base\n"
          ]
        }
      ],
      "source": [
        "print(f\"Loading model/tokenizer: {model_name}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "prefix = \"summarize: \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvJNWWh7Jq6Q"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    inputs = [prefix + doc for doc in examples[\"article\"]]\n",
        "    model_inputs = tokenizer(\n",
        "        inputs,\n",
        "        max_length=max_source_length,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "    # Target tokenization (new API)\n",
        "    labels = tokenizer(\n",
        "        text_target=examples[\"highlights\"],\n",
        "        max_length=max_target_length,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "    # Mask pad tokens in labels with -100 so they don't contribute to loss\n",
        "    label_ids = []\n",
        "    for label in labels[\"input_ids\"]:\n",
        "        label_ids.append([lid if lid != tokenizer.pad_token_id else -100 for lid in label])\n",
        "    model_inputs[\"labels\"] = label_ids\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "966dec7bab6747838bd1cb1b9607f1eb",
            "bf86800c35f14da8b3a4219c0e534cea",
            "9ed32e700f86475497707f1dbbe231fa",
            "9b295917f61847d2bf36e625e88bf857",
            "33a4bc31aae34d91bdcd289a5c0a08de",
            "2ce67511f7b14759a79aa304d3437ad5",
            "602b7c9155044b03b0879cc4c4e6d7bd",
            "cf69fff4409846b4a939463db2c30481",
            "2fe9a0e29cd74913a04d97fc2ea87171",
            "eabfe4cf257748ef8e6a548029c786b6",
            "a700dec49c414c1093b35ec15adb3273"
          ]
        },
        "id": "esTq0N92JuAL",
        "outputId": "982ccb04-53eb-46a4-96ab-e62092f0e5dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing dataset:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "966dec7bab6747838bd1cb1b9607f1eb"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "print(\"Tokenizing...\")\n",
        "tokenized = raw_subset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=raw_subset[\"train\"].column_names,\n",
        "    num_proc=NUM_PROC,\n",
        "    desc=\"Tokenizing dataset\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhMkMM9-JxYh"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    pad_to_multiple_of=8,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSbP7a1CJ1xZ",
        "outputId": "7eaefac8-cd09-40ae-a08b-689be06534cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading ROUGE metric...\n",
            "Loading BLEU metric...\n"
          ]
        }
      ],
      "source": [
        "import evaluate\n",
        "\n",
        "print(\"\\nLoading ROUGE metric...\")\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "print(\"Loading BLEU metric...\")\n",
        "bleu = evaluate.load(\"bleu\")   # or \"sacrebleu\" if you prefer\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "\n",
        "    # Replace -100 with pad_token_id for decoding predictions\n",
        "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "    # Replace -100 with pad_token_id for decoding references\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Strip whitespace\n",
        "    decoded_preds = [p.strip() for p in decoded_preds]\n",
        "    decoded_labels = [l.strip() for l in decoded_labels]\n",
        "\n",
        "    # Sentence-split with newlines for ROUGE-Lsum style\n",
        "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in decoded_preds]\n",
        "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in decoded_labels]\n",
        "\n",
        "    # ---- ROUGE ----\n",
        "    result = rouge.compute(\n",
        "        predictions=decoded_preds,\n",
        "        references=decoded_labels,\n",
        "        use_stemmer=True\n",
        "    )\n",
        "    result = {k: round(v * 100, 2) for k, v in result.items()}\n",
        "\n",
        "    # ---- BLEU ----\n",
        "    # evaluate's \"bleu\" expects references as List[List[str]] (list of refs per example)\n",
        "    bleu_result = bleu.compute(\n",
        "        predictions=decoded_preds,\n",
        "        references=[[ref] for ref in decoded_labels]\n",
        "    )\n",
        "    # bleu_result[\"bleu\"] is already in [0, 100] like sacrebleu\n",
        "    result[\"bleu\"] = round(bleu_result[\"bleu\"]*100, 2)\n",
        "\n",
        "    # Add \"eval_\" prefix for Trainer\n",
        "    return {f\"eval_{k}\": v for k, v in result.items()}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWOBO2DDJ4tz"
      },
      "outputs": [],
      "source": [
        "output_dir = f\"t5-cnn-dm-{model_name.replace('/', '-')}\"\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    # --- keep only widely supported args ---\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
        "    gradient_accumulation_steps=grad_accum_steps,\n",
        "\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    lr_scheduler_type=lr_scheduler,\n",
        "\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=200,\n",
        "    #eval_strategy=\"epoch\",\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=2000,           # checkpoints still saved periodically\n",
        "    save_total_limit=2,        # if your version supports it; if not, remove this line\n",
        "    gradient_checkpointing = True,\n",
        "\n",
        "    fp16=(mixed_precision == \"fp16\"),\n",
        "    bf16=(mixed_precision == \"bf16\"),\n",
        "    #dataloader_num_workers=4,\n",
        "    report_to=[\"none\"],\n",
        "\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=val_max_target_length,\n",
        "    generation_num_beams=gen_num_beams,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vu77eOufJ7a1",
        "outputId": "7262ac6f-1bc6-455c-9b82-e5c173ab38e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3673763792.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        }
      ],
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized[\"train\"],\n",
        "    eval_dataset=tokenized[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "RgiJGBYEJ-Pu",
        "outputId": "314f8c5f-faa5-43e3-8dc6-c6a18d93fc13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting training...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='459' max='459' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [459/459 14:37, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.744700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.623700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "print(\"\\nStarting training...\")\n",
        "train_result = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYAJDfSApUAv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f01ef4ed-32a0-4e75-faab-109664984c3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  total_flos               =  4142930GF\n",
            "  train_loss               =     1.6797\n",
            "  train_runtime            = 0:14:39.63\n",
            "  train_samples_per_second =      8.305\n",
            "  train_steps_per_second   =      0.522\n"
          ]
        }
      ],
      "source": [
        "trainer.log_metrics(\"train\", train_result.metrics)\n",
        "trainer.save_metrics(\"train\", train_result.metrics)\n",
        "trainer.save_state() # Saves log history in trainer_state.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d38QW-nHpUAv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "6f85a9eb-4cbb-4cdf-9e29-bcbda07352b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating final model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 09:33]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_bleu               =       10.8\n",
            "  eval_loss               =     1.8761\n",
            "  eval_rouge1             =      34.16\n",
            "  eval_rouge2             =      14.73\n",
            "  eval_rougeL             =      25.01\n",
            "  eval_rougeLsum          =      31.13\n",
            "  eval_runtime            = 0:09:36.88\n",
            "  eval_samples_per_second =      0.867\n",
            "  eval_steps_per_second   =      0.433\n",
            "Final eval metrics: {'eval_rouge1': 34.16, 'eval_rouge2': 14.73, 'eval_rougeL': 25.01, 'eval_rougeLsum': 31.13, 'eval_bleu': 10.8, 'eval_loss': 1.8760634660720825, 'eval_runtime': 576.8858, 'eval_samples_per_second': 0.867, 'eval_steps_per_second': 0.433, 'epoch': 3.0}\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nEvaluating final model...\")\n",
        "metrics = trainer.evaluate(\n",
        "    max_length=val_max_target_length, # Use consistent generation settings\n",
        "    num_beams=gen_num_beams\n",
        ")\n",
        "trainer.log_metrics(\"eval\", metrics)\n",
        "trainer.save_metrics(\"eval\", metrics)\n",
        "\n",
        "print(\"Final eval metrics:\", metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zB8HGpxHpUAw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b514af9-e612-4a5f-a5ba-ce5abbd42033"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saving final model...\n",
            "\n",
            "Final eval metrics: {'eval_rouge1': 34.16, 'eval_rouge2': 14.73, 'eval_rougeL': 25.01, 'eval_rougeLsum': 31.13, 'eval_bleu': 10.8, 'eval_loss': 1.8760634660720825, 'eval_runtime': 576.8858, 'eval_samples_per_second': 0.867, 'eval_steps_per_second': 0.433, 'epoch': 3.0}\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nSaving final model...\")\n",
        "trainer.save_model()\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "print(\"\\nFinal eval metrics:\", metrics)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nLoading ROUGE and BLEU metrics for TEST evaluation...\")\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "\n",
        "\n",
        "def test_compute(preds_ids, label_ids, tokenizer):\n",
        "    if isinstance(preds_ids, tuple):\n",
        "        preds_ids = preds_ids[0]\n",
        "\n",
        "    # Replace -100 with pad token for decoding\n",
        "    preds_ids = np.where(preds_ids != -100, preds_ids, tokenizer.pad_token_id)\n",
        "    label_ids = np.where(label_ids != -100, label_ids, tokenizer.pad_token_id)\n",
        "\n",
        "    decoded_preds = tokenizer.batch_decode(preds_ids, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "    decoded_preds = [p.strip() for p in decoded_preds]\n",
        "    decoded_labels = [l.strip() for l in decoded_labels]\n",
        "\n",
        "    # -------- ROUGE (aggregate) --------\n",
        "    preds_for_rouge = [\n",
        "        \"\\n\".join(nltk.sent_tokenize(p)) if p else \"\" for p in decoded_preds\n",
        "    ]\n",
        "    refs_for_rouge = [\n",
        "        \"\\n\".join(nltk.sent_tokenize(r)) if r else \"\" for r in decoded_labels\n",
        "    ]\n",
        "\n",
        "    rouge_result = rouge.compute(\n",
        "        predictions=preds_for_rouge,\n",
        "        references=refs_for_rouge,\n",
        "        use_stemmer=True,\n",
        "    )\n",
        "\n",
        "    #BLEU (aggregate)\n",
        "    bleu_result = bleu.compute(\n",
        "        predictions=decoded_preds,\n",
        "        references=[[r] for r in decoded_labels]\n",
        "    )\n",
        "\n",
        "    metrics = {\n",
        "        \"rouge1\": round(rouge_result[\"rouge1\"] * 100, 2),\n",
        "        \"rouge2\": round(rouge_result[\"rouge2\"] * 100, 2),\n",
        "        \"rougeL\": round(rouge_result[\"rougeL\"] * 100, 2),\n",
        "        \"rougeLsum\": round(rouge_result.get(\"rougeLsum\", 0.0) * 100, 2),\n",
        "        \"bleu\": round(bleu_result[\"bleu\"] * 100, 2),\n",
        "    }\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "qmpC0tF7uFZk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83fe3892-e838-4a19-e986-329528745261"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading ROUGE and BLEU metrics for TEST evaluation...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nGenerating predictions on TEST set...\")\n",
        "\n",
        "test_output = trainer.predict(\n",
        "    test_dataset=tokenized[\"test\"],\n",
        "    max_length=val_max_target_length,\n",
        "    num_beams=gen_num_beams,\n",
        ")\n",
        "\n",
        "test_metrics = test_compute(\n",
        "    preds_ids=test_output.predictions,\n",
        "    label_ids=test_output.label_ids,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "print(test_metrics)"
      ],
      "metadata": {
        "id": "1NJJRiG-uKJm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "53bc2fba-0bef-4229-f32b-2f5bd5165068"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating predictions on TEST set...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rouge1': np.float64(33.33), 'rouge2': np.float64(13.63), 'rougeL': np.float64(24.37), 'rougeLsum': np.float64(30.42), 'bleu': 10.08}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (NLP Env 3.11)",
      "language": "python",
      "name": "my_nlp_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "966dec7bab6747838bd1cb1b9607f1eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bf86800c35f14da8b3a4219c0e534cea",
              "IPY_MODEL_9ed32e700f86475497707f1dbbe231fa",
              "IPY_MODEL_9b295917f61847d2bf36e625e88bf857"
            ],
            "layout": "IPY_MODEL_33a4bc31aae34d91bdcd289a5c0a08de"
          }
        },
        "bf86800c35f14da8b3a4219c0e534cea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ce67511f7b14759a79aa304d3437ad5",
            "placeholder": "​",
            "style": "IPY_MODEL_602b7c9155044b03b0879cc4c4e6d7bd",
            "value": "Tokenizing dataset: 100%"
          }
        },
        "9ed32e700f86475497707f1dbbe231fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf69fff4409846b4a939463db2c30481",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2fe9a0e29cd74913a04d97fc2ea87171",
            "value": 500
          }
        },
        "9b295917f61847d2bf36e625e88bf857": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eabfe4cf257748ef8e6a548029c786b6",
            "placeholder": "​",
            "style": "IPY_MODEL_a700dec49c414c1093b35ec15adb3273",
            "value": " 500/500 [00:03&lt;00:00, 167.13 examples/s]"
          }
        },
        "33a4bc31aae34d91bdcd289a5c0a08de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ce67511f7b14759a79aa304d3437ad5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "602b7c9155044b03b0879cc4c4e6d7bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cf69fff4409846b4a939463db2c30481": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fe9a0e29cd74913a04d97fc2ea87171": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eabfe4cf257748ef8e6a548029c786b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a700dec49c414c1093b35ec15adb3273": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}